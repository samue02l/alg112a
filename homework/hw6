// 有使用ChatGPT進行輔助，并且有使用老師提供的程式

import numpy as np

def df(f, p, k, step=0.0001):
    p1 = p.copy()
    p1[k] += step
    return (f(p1) - f(p)) / step

def grad(f, p, step=0.0001):
    return np.array([df(f, p, k, step) for k in range(len(p))])

def gradient_descent(f, p0, alpha=0.01, eps=1e-6, max_iter=1000):
    p = p0.copy()
    for _ in range(max_iter):
        gp = grad(f, p)
        p_new = p - alpha * gp
        if np.linalg.norm(p_new - p) < eps:
            return p_new
        p = p_new
    return p

# example usage
def f(x):
    return (x[0] - 2) ** 2 + (x[1] + 3) ** 2 + (x[2] - 1) ** 2

p0 = np.array([1, -1, 2])
p_min = gradient_descent(f, p0)
print("Minimum point:", p_min)
print("Minimum value:", f(p_min))

